Building DAG of jobs...
Using shell: /usr/bin/bash
Provided remote nodes: 1
Provided resources: mem_mb=40000, mem_mib=38147, disk_mb=1000, disk_mib=954, time=1440
Select jobs to execute...
Execute 1 jobs...

[Wed Sep 11 12:01:39 2024]
rule cellranger_count:
    input: CSL-15900_NULL_OTH.txt
    output: CSL-15900_NULL_OTH/CSL-15900_NULL_OTH.mri.tgz
    log: logs/cellranger_count/CSL-15900_NULL_OTH.log
    jobid: 0
    reason: Forced execution
    wildcards: sample=CSL-15900_NULL_OTH
    threads: 20
    resources: mem_mb=40000, mem_mib=38147, disk_mb=1000, disk_mib=954, tmpdir=<TBD>, runtime=1440, time=1440

Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 20
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=40000, mem_mib=38147, disk_mb=1000, disk_mib=954, time=1440
Select jobs to execute...
Execute 1 jobs...

[Wed Sep 11 12:01:40 2024]
localrule cellranger_count:
    input: CSL-15900_NULL_OTH.txt
    output: CSL-15900_NULL_OTH/CSL-15900_NULL_OTH.mri.tgz
    log: logs/cellranger_count/CSL-15900_NULL_OTH.log
    jobid: 0
    reason: Forced execution
    wildcards: sample=CSL-15900_NULL_OTH
    threads: 20
    resources: mem_mb=40000, mem_mib=38147, disk_mb=1000, disk_mib=954, tmpdir=/tmp, runtime=1440, time=1440



Martian Runtime - v4.0.12

RuntimeError: pipestance 'CSL-15900_NULL_OTH' already exists and is locked by another Martian instance. If you are sure no other Martian instance is running, delete the _lock file in /scratch/gpfs/bjarnold/Graham/scripts/03_cellranger_count/CSL-15900_NULL_OTH and start Martian again.

2024-09-11 12:01:41 Shutting down.
Saving pipestance info to "CSL-15900_NULL_OTH/CSL-15900_NULL_OTH.mri.tgz"
For assistance, upload this file to 10x Genomics by running:

cellranger upload <your_email> "CSL-15900_NULL_OTH/CSL-15900_NULL_OTH.mri.tgz"

[Wed Sep 11 12:01:45 2024]
Error in rule cellranger_count:
    jobid: 0
    input: CSL-15900_NULL_OTH.txt
    output: CSL-15900_NULL_OTH/CSL-15900_NULL_OTH.mri.tgz
    log: logs/cellranger_count/CSL-15900_NULL_OTH.log (check log file(s) for error details)
    shell:
        
        /scratch/gpfs/bjarnold/Graham/cellranger/cellranger-8.0.1/cellranger count         --id CSL-15900_NULL_OTH         --transcriptome /scratch/gpfs/bjarnold/Graham/genomes_10X/ca_sea_lion         --create-bam true         --fastqs /scratch/gpfs/bjarnold/Graham/data_10X/cMaZuBaKFBP0Ygrh2ubrWr5mn7xa55w/20240821_ccc7_22G7Y5LT4_RQ25573_5p         --sample CSL-15900_NULL_OTH         --force-cells 18000         --include-introns true        --localcores 20         
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job cellranger_count since they might be corrupted:
CSL-15900_NULL_OTH/CSL-15900_NULL_OTH.mri.tgz
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Storing output in storage.
WorkflowError:
At least one job did not complete successfully.
DATA_DIR /scratch/gpfs/bjarnold/Graham/data_10X/cMaZuBaKFBP0Ygrh2ubrWr5mn7xa55w/20240821_ccc7_22G7Y5LT4_RQ25573_5p
srun: error: della-r4c1n1: task 0: Exited with exit code 1
srun: Terminating StepId=58872425.0
[Wed Sep 11 12:01:45 2024]
Error in rule cellranger_count:
    jobid: 0
    input: CSL-15900_NULL_OTH.txt
    output: CSL-15900_NULL_OTH/CSL-15900_NULL_OTH.mri.tgz
    log: logs/cellranger_count/CSL-15900_NULL_OTH.log (check log file(s) for error details)
    shell:
        
        /scratch/gpfs/bjarnold/Graham/cellranger/cellranger-8.0.1/cellranger count         --id CSL-15900_NULL_OTH         --transcriptome /scratch/gpfs/bjarnold/Graham/genomes_10X/ca_sea_lion         --create-bam true         --fastqs /scratch/gpfs/bjarnold/Graham/data_10X/cMaZuBaKFBP0Ygrh2ubrWr5mn7xa55w/20240821_ccc7_22G7Y5LT4_RQ25573_5p         --sample CSL-15900_NULL_OTH         --force-cells 18000         --include-introns true        --localcores 20         
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Storing output in storage.
WorkflowError:
At least one job did not complete successfully.
DATA_DIR /scratch/gpfs/bjarnold/Graham/data_10X/cMaZuBaKFBP0Ygrh2ubrWr5mn7xa55w/20240821_ccc7_22G7Y5LT4_RQ25573_5p
